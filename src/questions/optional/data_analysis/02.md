# 2. Hledání častých vzorů v datech (základní principy, metody, varianty, implementace)

**Labeled data**: There is a specially designated attribute and the aim is to use the data given to predict the value of that
attribute for instances that have not yet been seen.

**Supervised learning**: It is data mining using labeled data. It contains classification and numeric prediction.
Využití: klasifikace (A hospital may want to classify medical patients into those who are at high, medium or low risk of
acquiring a certain illness.), numerická predikce (A company wishes to predict a numeric value, such as a company’s
profits or a share price. )

**Unlabeled data**: Data that does not have any specially designated attribute.

**Unsupervised learning** (učení bez učitele): It is data mining using unlabeled data. The aim is to extract the most
information we can from the data available. It contains association rules and clustering.
Využití: asociační pravidala (If we know the purchases made by all the customers at a store for say a week, we may be
able to find relationships that will help the store market its products more effectively in the future), clustering (An
insurance company might group customers according to income, age, types of policy purchased or prior claims
experience. )

**Anonymization**: It is a process of removing personally identifiable information from datasets

### Asociační pravidla (Platoš)
- pravidla musí rozlišovat mezi třídními proměnnými
- Nechť X a Y jsou 2 sety itemů. Potom je X ⇒ Y asociačním pravidlem s minimálním supportem minsup a minimální spolehlivostní minconf, jestliže splňuje obě kritéria:
  - 1. Support pro X ∪ Y je minimálně minsup
  - 2. Spolehlivost pro X ⇒ Y je minimálně minconf.
- Postup:
  - 1. fáze: generují se všechny itemsety splňující minsup a minconf (možný apriori)
  - 2. fáze: generovat všechna pravidla splňující minconf – mějme všechny frekventované itemsety F, pro každý itemset I ∈ F generujeme všechny možné kombinace X a Y a počítáme konfidenci
- detekce outlierů
  - hledá se transakce, která není obsažena ve vzoru
  - vhodné pokud metody využívající vzdálenost nefungují

### Association Pattern Mining (Data mining pomocí asociačních pravidel)
- nejprve se provede jednoduchá statistika
- je možné vyzkoušet všechny kombinace – $2^n$ – hrozně moc
- další možnost vyjít z itemů, které se objevují často – 20%
- abychom našli pravidla, potřebujeme porovnat frekvenci výskytů potomků a jejich rodičů
- reprezentace může být binární, numerická, zero-position…
![](/images/ad_02_1.PNG)
![](/images/ad_02_2.PNG)

### Association Rule Generation Framework
Jde o if/then tvrzení, která pomáhají odkrýt vztahy mezi daty v relačních databázích nebo jiných informačních uložištích.
Příklad: ”If a customer buys a dozen eggs, he is 80% likely to also purchase milk.” X ⇒ Y

**Support** itemsetu I, sup(I), je definovaná jako část transakcí v databázi T = T1 , . . . $T_n$ která obsahuje I jako subset.

**Konfidence** (spolehlivost) -

**Monotonnost spolehlivost** (confidence monotonicity) - Let X1 , X2 and I be itemsets such that X1 ⊂ X2 ⊂ I. Then the
confidence of X2 ⇒ I − X2 is at least that of X1 ⇒ I − X1 .

**Frequent Itemset Mining** Given a set of transactions T = T1 , . . . Tn, where each transaction Ti is a subset of items
from U, determine all itemsets I that occur in a subset of at least a predefined fraction minsup of the transactions in T.

**Frequent Itemset Mining: Set-wise** Definition Given a set of sets T = T1 , . . . Tn, where each element of the set Ti is
drawn on the universe of elements U, determine all sets I that occur as a subset of at least a predefined fraction minsup
of the sets in T.

**Downward Closure Property** Every subset of a frequent itemset is also frequent

### Hledání supportu:

#### brute force – vygeneruju všechny, všechny spočítám

#### generuju od délky 1 a zvyšuju délku
- generuju lexikograficky seřazené
- použiju Downward closure property
- ořežu o irelevantní transakce
- využití kompaktní struktury dat pro kandidáty databáze a pro transakční databáze

#### apriori
- využívá downward closure property
- level-wise generation of candidates
  - kandidáti délky k jsou generováni
  - support je spočítán
  - kandidáti délky k+1 jsou generováni
  - k+1 itemsety jsou generovány jen pokud všechny usety jsou frekventované

![](/images/ad_02_3.PNG)

### Enumeration-Tree Algorithm
- Let I = {i1, . . . , ik} be a frequent itemset, where i1, i2, . . . , ik are listed in lexicographic order. The parent of the node I is the itemset {i1, . . . , ik1}. Thus, the child of a node can only be extended with items occurring lexicographically after all items occurring in that node.
- The enumeration tree can also be viewed as a prefix tree on the lexicographically ordered string representation of the itemsets.

![](/images/ad_02_4.PNG)

### TreeProjection
- A general framework for database projection (a mapping of a set of transaction to the itemset).
- The main idea follows the same properties that are used in Apriori.
- If a transaction does not contain itemset that corresponds to the node in enumeration tree, it will not be relevant even for the child nodes of the node.
- The proper selection of the node P for extension affect the memory consumption.
- Evaluate the Depth-first and Breath-first approach.
- The counting may be solved differently at deeper levels (such as bit maps)

### Interesting patterns = frequent itemset.
When we are investigating the relation between set of items, we are focused on the similarity more than on their
frequency. The Negative pattern mining is also difficult to find and investigate. New methods for two or more items to be
compared have to be defined.

#### Pearson coefficient of correlation between pair of items
- Holds the bit symmetry property $\rho_{ij} = \frac{sup(\{i,j\}) - sup(i) \cdot sup(j)}{\sqrt{sup(i) \cdot sup(j) \cdot (1 - sup(i)) \cdot (1 - sup(j))}}$
- měří vztah mezi i a j
- hodnoty jsou mezi $[−1, 1]$: +1 is a maximum positive correlation; −1 a maximum negative correlation; around 0 – weakly correlated data

#### χ2 Measure (chí kvadrát)
- Holds the bit symmetry property. $\chi^2(X) = \sum_{i=0}^{i<2^{|X|}} \frac{(O_i - E_i)^2}{E_i}$
- X je set k binárních itemů, počet všech možných kombinací je $2^{|X|}$.
- $E_i$ je očekávaná „fractional presence“ (jak česky?), když jsou itemy vzájemně nezávislé.
- The $O_i$ je pozorovaný výskyt, např. support kombinací $X_i$ itemů.
- χ2 blízko nule znamená, že prvky jsou nezávislé. Větší χ2 znamená vysokou závislost, ale neurčí, jestli positivní nebo negativní.

#### Interest ratio (ration = koeficient)
- Holds the bit symmetry property. $I(\{ i_1, \dots,i_k \}) = \frac{sup(\{ i_1, \dots, i_k \})}{\prod_{j=1}^k sup(i_j)}$
- Pro statisticky nezávislé itemy je společný support rovný součinu supportů jednotlivých itemů.
- Hodnota větší než 1 udává positivní korelaci, nižší než 1 negativní.
- Itemy, které se vyskytují výrazně méně (třeba jeden výskyt ve velké databázi) mohou koeficient zmást.
#### Symmetric Confidence Measures
- Klasická confidence measure je asimetrická vzhledem k antecedentu a consequentu.
- Míra supportu je symetrická. Symmetrická confidence může nahradit support-confidence s jedinou mírou.
- měření nesplňuje dolní uzávěrovou vlastnost (???)
#### Cosine coefficient (kosinova podobnost?) na sloupcích
- Measures the similarity between columns instead of rows. $cosine(i,j) = \frac{sup(\{ i,j \})}{\sqrt{sup(\{ i \})} \cdot \sqrt{\{ j \}}}$
- It may be viewed as a geometric mean of the confidences of the rules {i} ⇒ {j} and {j} ⇒ {i}.
#### Jaccard Coefficient
- Defined over sets. The sets are the transactions Ids in single columns. $J(S_1, S_2) = \frac{|S_1 \cap S_2 |}{|S_1 \cup S_2 |}$
- Satisfies the downward closure property.
#### Collective Strength
I je itemset. Míra je definovaná mírou narušení. Itemset I je v rozporu s transakcí, jestli některé itemy jsou přítomné v
transakci a jiné ne. Míra narušení v(I) je narušení itemsetu I přes všechny transakce.
Očekávaná hodnota E [v(i)] určuje statistickou nezávislost.

$C(I) = \frac{1-v(i)}{1-E[v(i)]} \cdot \frac{E[v(i)]}{v(i)}$ a $E[v(i)] = 1 - \prod_{u \in I} p_i - \prod_{u \in I} (1 - p_i)$
