# 8. Neuronové sítě (základní princip, metody učení, aktivační funkce)

### Neuronová síť se zkládá z uzlů - neuronů, které slouží jako výpočetní jednotka
- přijímají inputy z jiných neuronů nebo ze vstupní vrstvy, zpracují je a nastaví svůj výstup
- výpočet je ovlivněn váhami na vstupu a aktivační funkcí
- váhy jsou analogické k síle synapse v biologickém mozku a mění se v průběhu učení
- schopnost sítě se učit je založena na architektuře sítě:
  - jednovrstvé, vícevrstvé
  - rekurentní
  - Kohonenovy mapy - samoorganizující se mapy
  - convolution networks
  - deep neural networks
- učení probíhá předložením testovacích instancí a korekce odpovědí tím, že se upraví váhy
![](/images/ad_08_1.PNG)
### Jednovrstvá síť
- perceptron
  - většinou se myslí pouze jeden neuron viz obrázek, někdy se jako perceptron označují obecné jednovrstvé sítě
- základní architektura neuronek, struktura má dvě vrstvy:
  - vstupní - jeden “neuron” pro každý vstup, “neuron” jen předává hodnotu, nic nepočítá
  - výstupní uzel - neuron - vrací hodnotu {-1; 1} (definováno funkcí na neuronu)
- spojení mezi vstupními a výstupním uzlem tvoří váhy
- výstupní hodnota může být interpretována jako predikce třídní proměnné
- váhy W = {w1 , . . . ,wd} jsou upraveny pokud je predikovaná hodnota jiná než očekávaná
- funkcí je většinou lineární funkce (například vážený součet)
- každá váha je jedinečná pro konkrétní vstup do neuronu - tak velký vstup jako je dimenze dat d
- b je bias spojený s aktivační funkcí, výsledek Zi {-1; 1} odpovídá záznamu s atributy Xi = (x 1 i , . . . , xd)
$Z_i = sign\{ \sum_{j=1}^d w_j x_i^j + b \} = sign\{ \overline{W} \cdot \overline{X_i} + b \}$
- rozdíl mezi predikovanou Zi a reálnou hodnotou yi je (yi − zi) ∈ {−2, 0, 2} - prediction error
- výsledek 0 znamená, že se strefil a nic se neupravuje
- jinak je nutné upravit vektor vah a bias v závislosti na velikosti chyby
- učící proces je iterativní, změna vah i-tého vstupu v t iteraci: $\overline{W^{t+1}} = \overline{W^t} + \eta(y_i - z_i)\overline{X_i} $
- η je learning rate který reguluje rychlost učení sítě
- pro umocněný precision error platí: $(y_i - z_i)^2 = (y_i - sign(\overline{W} \cdot \overline{X_i} - b))^2$
- změna se provádí po jednotlivých průchodech u každého itemu, ne přes celý dataset
- velikost η ovlivňuje rychlost konvergence a kvality řešení
  - vysoké hodnoty znamenají rychlou konvergenci, ale suboptimální řešení může být nalezeno
  - nízká hodnota znamená lepší výsledek ale pomalou konvergenci
  - v praxi se snižuje hodnota η se zvyšujícím se počtem provedených epoch
### Vícevrstvá síť
- jeden neuron v perceptronu může řešit jen lineární problémy
- vícevrstvé sítě přidávají skryté vrstvy mezi vstupní a výstupní vrstvu
- skryté vrstvy mohou tvořit různé topologie i z několika dalších vrstev - výstup jedné vrstvy je vstupem pro druhou vrstvu - feed forward network
  - uzly v jedné vrstvě jsou plně propojeny s uzly z předchozí vrstvy
  - perceptron je vlastně jednovrstvá feed-forward síť
  - počet vrstev a počet uzlů v každé vrstvě definuje uživatel
- standardní vícevrstvé sítě používají jen jednu skrytou vrstvu - tvoří dvouvrstvé feed-forward neuronové sítě
![](/images/ad_08_2.PNG)
- aktivační funkce - nejen lineární vážený součet, ale i logistická, sigmoid, hyperpolický tanges
  - sigmoid: $z_i = \sum_{j=1}^d w_j \frac{1}{1 + e^{-x_i^j}} + b$
![](/images/ad_08_3.PNG)
  - logistická: $\sigma = \frac{1}{1 + exp(-n)}$
![](/images/ad_08_4.PNG)
  - hyperbolický tangens
![](/images/ad_08_5.PNG)
- učící algoritmus
  - náročnější než u perceptronu - největší problém tvoří skrytá vrstva - není pro ni řečeno, co má vracet
  - nějaký druh feedbacku je nutný z následujících vrstev do předchozích - řeší to back-propagation
    - dopředné šíření
      - vstupy se pošlou do vstupních neuronů
      - spočtené hodnoty se propagují pomocí vah do dalších vrstev
      - výsledek je porovnán s očekávanou hodnotou a je určena chyba
    - zpětná fáze (back-propagation)
      - cílem je pozměnit váhy ve zpětném směru aby se snížila chyba, která se projevila ve výstupní vrstvě
      - chyba na vnitřních vrstvách je vznikla jako chyba vyvolaná předchozími váhami
      - chyba je znovu odhadnuta použitím gradientní metody, proces je komplikován použitím nelineárních funkcí n vnitřních uzlů
- vícevrstvé sítě jsou silnější než kernel SVM, protože jsou schopné rozeznat libovolné funkce
- nezachytí pouze hranice rozhodování náhodných tvarů, ale taky nekontinuální třídní distribuce s různými hranicemi v různých oblastech
- se zvyšujícím se počtem uzlů a vrstev, může být prakticky určena jakákoli funkce
- neuronové sítě jsou univerzálním prostředkem pro hledání funkčních odhadů
- tato všeobecnost přináší nové problémy:
  - design topologie představuje mnoho možností pro analýzu
  - vysoké číslo uzlů a vrstev poskytuje větší obecnost ale také zvyšuje riziko over-fitting
  - neurální sítě je těžké interpretovat způsob klasifikace
  - učící proces je pomalý a citlivý na chyby
  - velké sítě mají velmi pomalý učící proces

### Rekurentní sítě
- propojení neuronů tvoří orientované cykly, což umožňuje dočasné chování nebo paměť
- takové sítě jsou schopné řešit sekvenci jako neizolovaný set
- trénování probíhá pomocí gradient descent nebo globálních optimizačních technik
- několik typů:
  - plně rekurentní - rekurentní spoje v každé vrstvě a aktivační funkce měnící se v čase
![](/images/ad_08_6.PNG)
  - rekursivní síť - některé váhy aplikuje rekurzivně přes grafovou strukturu, vhodné pro reprezentaci logických termů (ne všechny inputy jdou hned do první vrstvy)
![](/images/ad_08_7.PNG)
  - Hopfieldova síť - propojení každý s každým, jde o robustní adresovatelnou paměť
![](/images/ad_08_9.PNG)
  - Echo state network - speciální RNN která řídce spojuje náhodné vnitřní vrstvy, měnit se mohou pouze váhy ve výstupní vrstvě
![](/images/ad_08_8.PNG)

### Self-organizing map + convolutional nn 
- Platoš mad3 přednáška 5
