# 7. Support Vector Machines (princip, algoritmus, kernel trick)

- binární klasifikátor numerických dat
- multi-class generalizace je možná použitím několika různých strategií
- kategorické atributy jsou binarizovány
- třídní labely jsou z rozmezí (-1; 1)
- **hyperplane** - podprostor s menší dimenzí (ve 3D je hyperplane 2D)
- oddělovací hyperplanes jsou použity jako klasifikační kritérium
- hyperplane je definovaný pojmem “margin”

![](/images/ad_07.PNG)

- **lineárně separovatelné problémy**
  - hyperplane čistě rozdělí body patřící k jednotlivých třídám
  - existuje nekonečně mnoho možností, jak vytvořit lineární hyperplane mezi třídami
  - margin musí být co největší, je definován jako součet vzdáleností k nejbližším bodům z každé třídy
- margin na obou stranách hyperplane je stejný
- mohou být sestrojeny paralelní hyperplanes které se dotýkají trénovacích bodů z každé třídy a nemají žádné body mezi sebou
- trénovací body těchto hyperplanes jsou tvoří **support vectors** - vzdálenost mezi vektory je margin, oddělovací plocha je uprostřed těchto hyperplanes, abychom docílili co nejpřesnější klasifikace

- definice maximálního marginu
  - abychom mohli maximalizovat margin, musíme vyřešit problém nelineární programovací optimalizace a
vyjádřit margin jako funci využívající koeficientů hyperplane (???)
  - optimální koeficienty mohou být definovány vyřešením tohoto optimalizačního problému $\vec{W} \cdot \vec{X}+b=0$
- definice
  - n je počet bodů v training setu D
  - itý bod je označen jako $(X_i, y_i)$, kde $X_i$ je d-dimensionální řádek vektoru a $y_i$ je binární hodnota {-1; 1}
  - $\vec{W} = (w_1, ..., w_d)$ je d-dimensionální řádek vektoru reprezentující směr normály hyperplane
  - $b$ je číslo známé jako bias
  - problém: zjištění (d+1) koeficientů odpovídajících $\vec{W}$ a $b$ z trénovaích dat, která maximalizují margin
    - každý bod leží na opačné straně hyperplane ... z toho odvodíme: $y_i (\vec{W} \cdot \vec{X_i}+b) \geq + 1$
    - cílem je maximalizovat vzdálenost mezi paralelními hyperplany: $max: \frac{2}{\left \| \vec{W} \right \|} = \frac{2}{\sqrt{\sum_{i=1}^{d} w^2_i}}$
    - odpovídá: $min: \frac{\left \| \vec{W} \right \|^2}{2}$
    - minimalizace druhého vzorce je komplexní kvadratický programovací problém protože parametr je minimalizovaný v závilosti na souboru lineárních omezení (viz první bod)
    - každý bod vede k omezení, proto je SVM výpočetně složité
    - jednou z možností je vyřešit problém jako Lagrangian relaxation
    - ke každému omezení určí ne-negativního násobitele $λ = (λ_1 , ..., λ_n)$
    - omezení jsou pak uvolněna a objektivní funkce je posílena začleněním Lagrangianovy penalizace za narušení omezení
    - $L_P = \frac{\left \| \vec{W} \right \|^2}{2} - \sum_{i=1}^n\lambda_i \left [ y_i(\vec{W} \cdot \vec{X_i}+b)-1\right ]$
    - konverze $L_P$ na striktně čistý maximalizační problém eliminací minimalizační části
    - proměnné $W$ a $b$ jsou převedeny na gradient-based podmínku a nastaveny na 0
    - $\nabla L_P = \nabla \frac{\left \| \vec{W} \right \|^2}{2} - \nabla \sum_{i=1}^n\lambda_i \left [ y_i(\vec{W} \cdot \vec{X_i}+b)-1\right ] = 0$
    - $\vec{W} - \sum_{i=1}^n \lambda_i y_i \vec{X_i} = 0$
    - odvození W: $\vec{W} = \sum_{i=1}^n \lambda_i y_i \vec{X_i}$ odvození b: \sum_{i=1}^n \lambda_i y_i = 0
    - finální lagrangian: $L_D = \sum_{i=0}^{n}\lambda_i - \frac{1}{2}\sum_{i=1}^n\sum_{j=1}^n\lambda_i\lambda_j y_i y_j \vec{X_i} \vec{X_j}$
    - třídní label pro instanci Z: $F(\vec{Z}) = sign\left \{ \vec{W} \cdot \vec{Z} + b \right \} = sign \left \{ (\sum_{i=1}^n\lambda_i y_i \vec{X_i} \cdot \vec{Z}) + b \right \}$

    - $L_D$ se řeší pomocí stoupání gradientu vzhledem k parametru vektoru $\lambda$

  - soft margin pro lineární neseparovatelná data
    - margin je definován jako soft s penalizací marginových omezení narušení
    - definice Lagrangiana je velmi podobná Lagrangianovi pro separovatelné případy s indukcí nových omezení na parametrech $\lambda$

  - Non-linear decision boundary (nelineární rozhodovací hranice)
    - ve skutečných případech nejsou hranice lineární
    - body mohou být transformovány do vyšších dimenzí, aby bylo možné určit lineární hranice
    - obrázek - body v 1D (přímka) se přenesou do 2D (hyperbola)
- **The Kernel Trick**
  - ovlivňuje důležitá pozorvání, která mohou být ve formulaci SVM plně vyřešena jako dot product (nebo podobnost - všude, kde je dot product je v závorce i podobnost) mezi páry bodů
  - hodnoty vlastností nejsou důležité nebo potřebné
  - klíčem je definovat párový dot product v d-dimensionální transformované reprezentaci Φ(X)
  - $K(\vec{X_i},\vec{X_j}) = \phi(\vec{X_i}) \cdot \phi(\vec{X_j})$
  - je vyžadován jen dot product, takže není důležité počítat vlastnosti transformovaného X
  - finální lagrangian: $L_D = \sum_{i=0}^{n}\lambda_i - \frac{1}{2}\sum_{i=1}^n\sum_{j=1}^n\lambda_i\lambda_j y_i y_j K(\vec{X_i},\vec{X_j})$
  - třída je vypočtena jako: $F(\vec{Z}) = sign\left \{ \vec{W} \cdot \vec{Z} + b \right \} = sign \left \{ (\sum_{i=1}^n\lambda_i y_i K(\vec{X_i},\vec{Z})) + b \right \}$
  - všechny výpočty probíhají v originálním prostoru
  - aktuální transformace Φ(·) není známá tak dlouho, dokud není známá kerlen similarity function K(·)
  - kernel function musí být zvolena opatrně, musí dodržet **Mercer’s theorem**:
    - říká, že n × n kernel matrix (taky Gramm matrix) $S = \left [ K(\vec{X_i},\vec{X_j}) \right ]$ je pozitivní semidefinitní
  - další aplikace kernelu:
    - kernel K-means
    - Kernel PCA - výměna dot productu za mean-centered matici
    - Kernel fisher Discriminant
    - Kernel Linear Discriminant Analysis

|            Function            |                        Form                             |
| ------------------------------ | -------------------------------------------------------- |
| Linear kernel                  | $K(\vec{X_i},\vec{X_j}) = \vec{X_i} \cdot \vec{X_j} + c$ |
| Polynomial kernel              | $K(\vec{X_i},\vec{X_j}) = (\alpha\vec{X_i} \cdot \vec{X_j} + c)^h$ |
| Gaussian radial basis function | $K(\vec{X_i},\vec{X_j}) = exp(-\frac{\left \| \vec{X_i} - \vec{X_j} \right \|^2}{2\sigma^2})$ |
| Sigmoid kernel                 | $K(\vec{X_i},\vec{X_j}) = tanh(\kappa \vec{X_i} \cdot \vec{X_j} - \sigma)$ |
| Exponential kernel             | $K(\vec{X_i},\vec{X_j}) = exp(-\frac{\left \| \vec{X_i} - \vec{X_j} \right \|}{2\sigma^2})$ |
| Laplacian kernel               | $K(\vec{X_i},\vec{X_j}) = exp(-\frac{\left \| \vec{X_i} - \vec{X_j} \right \|}{\sigma})$ |
| Rational Quadratic kernel      | $K(\vec{X_i},\vec{X_j}) = 1 - \frac{\left \| \vec{X_i} - \vec{X_j} \right \|^2}{\left \| \vec{X_i} - \vec{X_j} \right \|^2 + c}$ |
