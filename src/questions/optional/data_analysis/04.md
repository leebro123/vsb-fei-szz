# 4. Shlukování na základě hustoty, validace shluků, pokročilé metody shlukování (CLARANS,BIRCH, CURE)

## Shlukování na základě hustoty
- tvar a počet clusterů nemusí být definován
- snaží se najít místo s menší hustotou – podle nich clustruje
- může se považovat za dvouúrovňový hierarchický clustrovací algoritmus
- druhý lvl je detailnější díky nižšímu počtu bodů (single-linkage agglomerative algorithm)
### grid based algorithm
- data jsou rozdělena do p intervalů (většinou stejné délky), pro d-dimensionální dataset je vytvořeno $p^d$ hyper-cubes, mez hustoty τ udává počet hyper-cubes, které jsou husté(τ je minimální počet bodů v každé hyper-cube), clustery se tvoří spojováním hustých regionů
- dva sousední regiony mohou být spojeny pokud sdílí stranu nebo roh
- cílem je nalézt propojené regiony
- breadth-first or depth-first traversal
- počet clusterů není předdefinován, musí být definovaný rozsah mřížky p a mez hustoty (density threshold τ)
- příliš malé p promíchá body z různých clusterů, příliš velké p vyprodukuje hodně prázdných buněk
- příliš malé τ dá hodně hustých regionů včetně outlierů, příliš velké zmenší velikost clusteru nebo jejich počet
- dobré pokud hustota clusterů je podobná (globální τ)
### DBSCAN
- podobný grid-based metodě
- obdélníkové oblasti jsou substituovány za kulovité oblasi s radiusem Eps, Eps a τ musí být definovány, Eps by mělo být nastaveno tak, aby vyhodnotilo většinu bodů jako core
- body jsou klasifikovány do tří kategorií
  - Core point – má nejméně τ bodů v radiu Eps
  - Border point – má méně než τ bodů v okolí, ale má v okolí core point
  - Noise point
- nejprve jsou definovány core, border a noise body, pak se z nich vytvoří connectivity graf
- pospojují se ty core body, které mají mezi sebou vzdálenost menší než Eps
- border body jsou připojeny k těm clusterům, ke kterým mají největší konektivitu
- problém u clusterů s různou hustotou (τ globální)
### DENCLUE
- slidy Platoš 5. přednáška – shlukování na základě hustoty

## Validace shluků
- Interní validační kritéria
  - Sum of Square Distances to Centroids
  - Intra-cluster to Inter-cluster distance ratio.
  - Silhouette coefficient
  - Probabilistic measure
- External Validation Criteria
  - Purity
  - Gini index
  - Entropy

### Vnitřní validační kritéria
- užitečné, pokud nejsou dostupná externí kritéria
- hlavní problém při určování interních kritérií je to, že jsou závislé na použitém algoritmu
- hlavní použití kritérií je pro porovnání algoritmů ze stejné třídy nebo různých běhů stejného algoritmu
- ladění parametrů
  - každý algorimus má několik parametrů které musí být ručně nastaveny
  - interní míry mohou být použity pro zpřesnění definice těchto parametrů
#### Sum of Square Distances to Centroids
  - užitečné, pokud jsou centroidy definované (většinou vzdálenostní metody) $SSQ = \sum_{X \in D} dist(X,C)^2$
  - udává míru kvality – menší míra je lepší clustrovací kvalita
  - $C$ ve vzorci je nejbližší centroi k $X$
#### Intra-cluster to Inter-cluster distance ratio
  - založeno na setu náhodných párů objektů
  - $P$ je set párů z jednoho clusteru, $Q$ je set párů, které nejsou z jednoho clusteru, průměrná vzdálenost je definována vzorci: $Intra = \frac{1}{|P|} \sum_{(X_i, X_j) \in P} dist(X_i,X_j)$ a $Inter = \frac{1}{|Q|} \sum_{(X_i, X_j) \in Q} dist(X_i,X_j)$
  - poměr Intra/Inter je mírou kvality – menší hodnota = vyšší kvalita
#### Silhouette coefficient (silueta, obrys)
  - porovnává podobnost vzdáleností jako předchozí $S_i = \frac{Dmin_i^{out} - Davg_i^{in}}{max \{Dmin_i^{out} - Davg_i^{in}\}}$
  - $Davg_i^{in}$ je průměrná vzdálenost $X_i$ k bodům v clusteru
  - $Dmin_i^{out}$ je minimální z průměrných vzdáleností ke všem ostatním clusterům
  - celkový silhouette coefficient je průměr koeficientů specifických pro jednotlivé body
  - hodnota v intervalu (-1;1), positivní hodnoty - vysoce separované clustery, záporné - promíchání clusterů

### Externí validační kritéria
- ohodnocení externími kritérii je preferovanější
- clustery by měly být označeny štítky (labels), které by neměly odpovídat přirozeným clusterům
- počet přirozených clusterů by neměl odrážet počet tříd
- když je stejný počet clusterů a tříd, je užitečná confusion matrix
#### Cluster purity and class-based Gini index
- $m_{ij}$ představuje počet bodů ze třídy, která jsou namapovaná na cluster $j$
- počet správně zařazených bodů je značeno $N_i$ $N_i = \sum_{j=1}^{k_d} m_{ij} \quad \forall i = 1 \dots k_t$
- počet bodů bodů v clusteru je značeno $M_i$ $M_i = \sum_{j=1}^{k_t} m_{ij} \quad \forall i = 1 \dots k_d$
- $k_t$ je počet tříd, kd je počet clusterů
- kvalitní algoritmicky determinovaný cluster $j$ by měl obsahovat body z jedné třídy
- počet bodů z dominantní třídy v clusteru je definováno jako $P_j$ = $max_i\{m_{ij}\}$
- celková purita je definována podle vzorce, může být spočtena pro “ground truth” clusterů, průměr z těchto hodnot může být použit jako finální míra $Purity = \frac{\sum_{j=1}^{k_d} P_j}{\sum_{j=1}^{k_d} M_j}$
- purity ignoruje distribuci nedominantních tříd
- Gini index měří rozčlenění dat a zohledňuje i zastoupení nedominujících tříd $G_j = 1- \sum_{i=1}^{k_t} (\frac{m_{ij}}{M_j})^2$ a $G_{average} = \frac{\sum_{j=1}^{k_d} G_j \cdot M_j}{\sum_{j=1}^{k_d} M_j}$
- $G_j$ je malé když jsou hodnoty zkreslené, hodnoty blízko 1 − 1/$k_t$ když je distribuce stejnoměrná
- průměrný Gini koeficient je vážen počtem bodů $M_j$
#### Entropie
- měří stejnou charakteristiku jako Gini index $E_{average} = \frac{\sum_{j=1}^{k_d} E_j \cdot M_j}{\sum_{j=1}^{k_d} M_j}$ a $E_j = 1- \sum_{i=1}^{k_t} (\frac{m_{ij}}{M_j}) \cdot \log(\frac{m_{ij}}{M_j})$
- průměrná entropie je vážená počtem bodů $M_j$

## Pokročilé metody shlukování (CLARANS, BIRCH, CURE)
problémy při shlukování:
### Categorical data clustering
- problém určit průměr clusteru, těžká definice podobnosti
- používá se binarizace dat pro ulehčení výpočtů
#### representative-based clustering
  - centroid se hledá jako pravděpodobnost
  - podobnost centroidů
    - pro každý atribut se sečte pravděpodobnost jeho hodnoty
    - objekt je přiřazen tam, kde je nejpodobnější
    - funguje dobře pro nezkreslená data
    - Skewed data may be modified using weights that prefer rare items. The weights have to be incorporated into similarity measurement and histogram generation.

#### k-Modes clustering
  - je vybrán mode pro representativní atribut, mode je nejpravděpodobnější hodnota atributu v clusteru (nejčastěji jeden objekt z clusteru)
  - reprezentant je také kategorický objekt
  - mody by se neměly používat pro řídké údaje, fungují dobře pro rovnoměrně distribuovaná data, nerovnoměrně distribuovaná data musí být normalizována

#### hierarchické algoritmy - ROCK (robust clustering using links) (více Platoš 7. přednáška)

### Scalable clustering (škálovatelné)
- velké množství dat (nemohou být uložena do paměti), mnoho algoritmu potřebuje několik průchodů
#### CLARA
- škálovatelná implementace algoritmu k-medoids
- založen na Partitioning Around Medoids (PAM)
  - všechny možné (k · (k − n)) páry jsou ohodnoceny a nejlepší je vybrán (a pořád dokola)
  - O(kn2d) časová složitost pro d-dimensionální dataset.
- kvůli výpočetní náročnost je vybrán subset
- vzorkovací frakce je vybrána kde f << 1
- nevzorkované body jsou přiřazeny k nejbližším medoidům
- vzorkování se opakuje na nezávisle zvolených vzorcích stejné velikosti f*n
- nejlepší clustery jsou zvoleny
- Time complexity if for one iteration: $O(k \cdot f^2 \cdot n^2 \cdot d + k \cdot (n-k))$

#### CLARANS (Clustering Large Applications based on Randomized Search)
- řeší problém z CLARA, kde není žádná dobrá volba medoidu ve vzorku
- pracuje s celým datasetem, iterativně zkouší výměny mezi náhodnými medoidy a ne-medoidy
- kvalita výměny je zkontrolována po každém pokusu, pokud se zlepší, je změna potvrzena, pokud ne, zvýší se čítač neúspěšných výměn
- lokální optimum je nalezeno pokud počet neúspěšných výměn je uživatelem nastaený MaxAttempt
- toto se opakuje pro uživatelem specifikovaný počet iterací - MaxLocal
- nejlepší řešení je vyhodnoceno jako optimum
- výhoda oproti CLARA je větší diversita prostoru je prohledána

#### BIRCH (Balanced Iterative Reducing and Clustering using Hierarchies)
- kombinace top-down a k-means algoritmu
- specifická datová struktura CF-Tree
  - strom s vyrovnanou hloubkou
  - každý uzel má větvící faktor nanejvýš B
  - je navržen pro podporu dynamického vkládání
- každý kořen obsahuje součet všech subclusterů - “cluster feature” (CF) vektor
  - SS je vektor součtů mocnin bodů v clusteru (2nd order moment)
  - LS je vektor lineárních součtů bodů v clusteru (1st order moment)
  - m je počet bodů v clusteru
- vlastnosti
  - CF může být reprezentován lineárním součtem CF jednotlivých bodů
  - CF rodiče je součet CF potomků
  - CF spojených clusterů je suma CF jednotlivých clusterů
  - inkrementace CF vektoru může být dosažena přidám CF vektoru do clusteru
  - CF může být použito k počítání vlastnosti clusteru (radius a centroid).

![](/images/ad_04_1.PNG)

- každý list má prahový průměr T - reguluje granularitu, výšku stromu a souhrn clusterů v listech
- T je založeno na velikosti datasetu s ohledem na veliksot paměti (malé datasety s malý T) - pokud nemůže být strom udržem v paměti, zvýší se T
- nízké T vede k velkému počtu jemnozrnných clusterů
- vložení do stromu využívá top-down přístup - v každé úrovni je použit nejbližší centroid CF jsou aktualizovány, v listu, pokud je splněn T, je vložen na odpovídající místo, jinak je vytvořen nový cluster obsahující jen tento bod - může vyžadovat rozdělení, pokud dojde k nedostatku paměti, je strom přestavěn s vyšším T

#### CURE (Clustering using representatives)
- bottom-up aglomerativní hierarchický algoritmus
- místo počítání vzdáleností mezi všemi páry počítá vzdálenost mezi reprezentanty
- reprezentanti jsou vybíráni taky, aby zachovali tvar clusterů: jeden je nejvzdálenější od centra clusteru, druhý je nejvzdálenější od prvního, třetí nejvzdálenější od nejbližšího bodu těch dvou...
- reprezentanti by měli tvořit obrys clusteru, většinou vybráno malé množství z bodů v clustru
- tím, že se vybírá nejvzdálenější bod, se mohou vybrat i outlieři, aby se tomu zabránilo, vytváří se noví reprezentanti, jejichž délka od středu je zlomek té původní
- malé clustery, které se během clustrování nezvětšují, jsou eliminovány jako outlieři
- zlepšení složitosti můžeme docílit použitím náhodného vzorku datasetu a rozdělení tohoto vzorku, který je přeclusterován
- algoritmus:
  - vzorek bodů s z datasetu D velikosti n
  - rozdělení vzorku do p částí velikost s/p
  - nezávislé clustrování každé části s využitím hierarhického spojování s k clustery v každé části, celkový počet k*p clusterů přes všechny části je stále větší než uživatelem definovaný cíl k
  - hierarhické clustrování přes k*p clusterů přes všechny části do cílu k
  - přiřadit každý z nevzorkovaných bodů ke clusteru s nejbližšími reprezentanty

![](/images/ad_04_2.PNG)

- algoritmus podle Stanford University (youtube)
  - ----první část----
  - vyberu vzorek dat
  - vytvořím z něj clustery
  - vyberu reprezentanty:
    - pro každý cluster vyberu k reprezentantivů
    - posunu je o zlomek vzdálenost blíže k centroidu clutseru (např. 20%)
  - ---druhá část----
- každý bod X v datasetu vložím do nejbližšího clusteru - do clusteru, jehož reprezentativ je nejblíže bodu X

platoš slidy 8. přednáška
- high-dimensional clustering, CLIQUE, PROCLUS, ORCLUS
- Semi-supervised Clustering, Point-wise supervision,
- Visual clustering
- Clustering Ensembles

#### High-dimensionality clustering - hodně irelevantních atributů
- Semi-supervised clustering - částená informace o podřízených cluster, což může zlepšit výsledky
- Interactive and visual clustering - feedback od uživatele je použitýpro zlepšení výsledků, visualizace pomáhá porozumět datům
- Ensemble (soubor) clustering - využití více modelů pro clustrování, nejlepší model nebo jejich kombinace je použit
