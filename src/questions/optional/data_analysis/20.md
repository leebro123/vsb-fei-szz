# 20. Lineární algebra v DIS

Some additional info:

https://www.google.com/url?sa=t&rct=j&q=&esrc=s&source=web&cd=&cad=rja&uact=8&ved=2ahUKEwizsM6O_c33AhVEhf0HHTFVC4AQFnoECAMQAQ&url=http%3A%2F%2Fwww.cs.vsb.cz%2Fdateso%2F2004%2Fslides%2Fmoravec.pps&usg=AOvVaw03u85Ozlk7oiT_nKnge1O2

https://nlp.stanford.edu/IR-book/

### Metody redukce dimenze

Způsobem, jak snížit čas potřebný pro porovnání je snížení dimenze prostoru dokumentů. Toho lze
dosáhnout vypuštěním některých termů, avšak na úkor přesnosti a úplnosti. Proto redukujeme dimenzi
jinými způsoby. Nejznámější metodou je latentní sémantické indexování (LSI), při němž v většinou
počítáme k-redukovaný singulární rozklad matice termů v dokumentech (metoda SVD).Kromě redukce
dimenze, při níž je výsledná chyba nejmenší možná, získáme navíc tzv. latentní sémantiku – dokumenty
týkající se stejné tématické oblasti si budou blíže a vyniknou vztahy mezi významově (sémanticky)
podobnými termy. Bohužel současné metody výpočtu LSI jsou výpočetně náročné – i rychlá metoda
Lanczos má časovou složitost O(mnc), kde c je průměrný počet různých termů v dokumentu.Proto byly
hledány rychlejší metody, které by dosahovaly podobných výsledků za cenu horší chyby aproximace.
Jednou z těchto metod je náhodná projekce, která při dostatečně vysoké redukované dimenzi d, d << n
dobře zachovává vzdálenosti a úhly mezi vektory. Náhodná projekce je založena na násobení matice
termů v dokumentech maticí náhodné projekce R. Prvky R jsou náhodná čísla, jejichž distribuce má
nulovou střední hodnotu a jednotkový rozptyl – nejčastěji N(0, 1) a Achlioptasem navržené distribuce
${−√3, 0, +√3}$ s pravděpodobnostmi $2/3$ pro 0 a $1/6$ pro $√3, a −√3 a {−1, +1}$, obě hodnoty s
pravděpodobností $1/2$
(poskytuje o něco horší výsledky než předchozí metody). Testy ukazují, že např. pro m = 20000 a d ≥
500 jsou přesnost i úplnost velmi dobré a pohybují se nad 90%. Časová složitost náhodných projekcí
je nižší než u LSI – O(cdn), ale výsledné vektory dokumentů již nejsou řídké, nezískáme latentní
sémantiku a redukovaná dimenze není natolik malá, aby překonala „prokletí dimenze”. Je ale možno
použít náhodné projekce jako předstupně LSI – pak dochází k rychlejšímu výpočtu singulárního
rozkladu, časová složitost je $O(m(log2 n + c log n))$ a výsledný rozklad velmi dobře aproximuje
rozklad, získaný přímým výpočtem LSI.

### Rozklady matic

SVD (Singular Value Decomposition) je rozklad komplexní nebo reálné matice M na maticový součin
$UΣV^T$. Přitom $U$ je reálná nebo komplexní unitární matice o rozměrech $m×m$, $V$ je reálná nebo
komplexní unitární matice $n×n$ a $Σ$ je matice $m×n$ nulová až na případná nezáporná čísla na
hlavní diagonále; čísla na její hlavní diagonále se označují jako singulární hodnoty matice $M$.
Hvězdička označuje konjugovanou matici, tedy transponovanou matici komplexně sdružených prvků.
Požadujeme-li, aby singulární hodnoty byly seřazeny sestupně, je matice $Σ$ určena jednoznačně,
naopak matice $U$ a $V$ jednoznačné být nemusejí. Singulární rozklad vždy existuje a používá se k
řadě teoretických i praktických účelů. Lze ho chápat také jako zobecnění Schurova rozkladu na matice
obecného tvaru. Nevýhodou je, že výpočetní náročnost konstrukce singulárního rozkladu roste se třetí
mocninou rozměru matic.

Geometricky existence singulárního rozkladu znamená, že každý lineární operátor mezi reálnými
vektorovými prostory konečných dimenzí lze rozložit na rotaci vzorů (matice $V^T$), vynásobení (
části) rotovaných vektorů nezápornými koeficienty (singulárními hodnotami) a opětnou rotaci (
případně rotaci kombinovanou se zrcadlením) v prostoru obrazů (matice $U$). Anebo můžeme matice $U$
a $V$ interpretovat jako matice přechodu mezi bázemi a říci, že pro každý lineární operátor mezi
reálnými konečněrozměrnými vektorovými prostory lze najít dvojici ortonormálních bází (v prostoru
vzorů a v prostoru obrazů) tak, že daný operátor se v těchto bázích zapíše jako matice s nezápornými
čísly na hlavní diagonále a nulami všude jinde, tj. i-tou složku vzoru v první bázi násobí i-tou
singulární hodnotou, čímž získá i-tou složku zápisu obrazu ve druhé bázi.

![](https://upload.wikimedia.org/wikipedia/commons/thumb/c/c8/Singular_value_decomposition_visualisation.svg/800px-Singular_value_decomposition_visualisation.svg.png)

https://www.youtube.com/watch?v=gXbThCXjZFM

##### Použití:

- **Low-rank approximation** – je minimalizační problém, v němž loss function měří shodu mezi danou
  maticí (daty) a aproximační maticí (optimalizační proměnnou), přičemž platí omezení, že
  aproximační matice má sníženou hodnost.
- Obecně redukce dimenzionality
- Řešení lineárních rovnic
- Zpracování obrazu, signálů, big data..

### Latentní semantika

- Je technika, která zobrazuje dokumenty a dotazy do prostoru latentních sémantických dimenzí,
  přičemž slova, která jsou sémanticky podobná (měřeno mírou souvýskytů v dokumentech) jsou
  zobrazována do stejných dimenzí a slova sémanticky odlišná do různých dimenzí. Díky tomu mohou mít
  velkou sémantickou podobnost i dokumenty (případně dotaz a dokument), které spolu nesdílejí žádná
  slova.

##### Příklad:

Dokumenty:

1. the man walked the dog
2. the man took the dog to the park
3. the dog went to the park

Reprezentace ve vektorovém prostoru:

1. [1, 1, 0, 2, 0, 0, 1, 0]
2. [1, 1, 1, 3, 1, 1, 0, 0]
3. [1, 0, 1, 2, 1, 0, 0, 1]

- Pro nalezení konceptů, příslušností slov ke konceptům a míry příslušnosti konceptů k dokumentům se
  používá matematická metoda nazývaná **singular value decomposition** (SVD). Jediným potřebným
  vstupem je kolekce dokumentů a počet konceptů, které chceme identifikovat. Výhodou latentní
  sémantické analýzy je, že určuje i váhy konceptů pro celou kolekci trénovacích dokumentů. Tedy pro
  každý koncept vrací i reálné číslo, které udává, jak moc je daný koncept v poskytnuté kolekci
  dokumentů významný. Díky tomu je možné koncepty s nejnižší vahou ignorovat a díky tomu dále
  snižovat dimenzionalitu sémantického prostoru. Lze matematicky dokázat, že chyba, ke které dojde
  při zanedbání konceptu s minimální vahou, je nejmenší možná.

![](https://blog.seznam.cz/wp-content/uploads/2011/10/lsa_bild.jpg)

Do stejného prostoru je zobrazen i pseudodokument „the dog walked“. Je vidět, že pseudodokument je v
našem sémantickém prostoru nejblíže dokumentu 1. Stejně by to dopadlo i v původním 8-dimenzionálním
prostoru, zde nám ale stačily pouze 2 dimenze.

(Cílem SVD je najít optimální sadu faktorů, které nejlépe předpovídají výsledek. Během
předzpracování dat před operacemi text miningu se SVD používá v latentní sémantické analýze k
nalezení základního významu termínů v různých dokumentech. Když zpracováváme korpus textu, můžeme
jej reprezentovat jako matici **term x dokument**, tuto matici lze rozložit pomocí SVD, low-rank
approximation zachycuje (určitým způsobem) sémantiku v našem textu, což má obvykle několik výhod,
jako je filtrování šumu, řešení problému synonym atd. )

### Náhodná projekce

Náhodná projekce je metoda rozkladu, která se používá ke snížení dimenzionality vysokorozměrných
dat. Tyto techniky jsou často oceňovány pro svou sílu, jednoduchost a překvapivou účinnost při
vytváření nízké míry chyb. To je v porovnání s mnoha jinými metodami dekompozice, jako je například
dekompozice singulárních hodnot, o které jsme hovořili dříve. Základem vzniku této techniky a
většiny jejích implementací je něco, co se nazývá **Johnson-Lindenstraussovo lemma**
(_"Pokud jsou body ve vektorovém prostoru promítnuty do náhodně vybraného podprostoru s vhodně
velkými rozměry, pak jsou vzdálenosti mezi body přibližně zachovány."_).

##### Algoritmus

1. Vezměte soubor dat $K$ o rozměru $M x N$ (M=vzorky, N=původní rozměr).
2. Inicializujte náhodnou 2D matici $R$ o velikosti $N x D$, kde $D$ = nová redukovaná dimenze.
3. Normalizujte sloupce $R$ tak, aby byly vektory jednotkové délky.
4. Násobení matice $J = K * R$. $J$ je konečný výstup s rozměrem $M x D$.