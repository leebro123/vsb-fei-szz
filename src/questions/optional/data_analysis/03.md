# 3. Shlukovací metody (shlukování pomocí reprezentantů, hierarchické shlukování)
Shluková analýza (též clusterová analýza, anglicky cluster analysis) je vícerozměrná statistická metoda, která se používá ke klasifikaci objektů. Slouží k třídění jednotek do skupin (shluků) tak, aby si jednotky náležící do stejné skupiny byly podobnější než objekty z ostatních skupin.

## Shlukování pomocí reprezentantů
- nejjednodušší algoritmus založený na vzdálenosti/podobnosti mezi body
- clustery jsou tvořeny v jednom kroku, mezi různými clustery neexistují vztahy
- pro přiřazení bodu do clusteru se využívá vzdálenost (distance function)
- počet clusterů k je definován obvykle uživatelem
- dataset D obsahuje n bodů X1, ..., Xn
- cílem je určit takové Y1, ..., Yk které minimalizují fci: $O = \sum_{i=1}^{n}[min_jDist(X_i, Y_i)]$
- součet vzdáleností různých bodů k jejich nejbližšímu reprezentantovi musí být minimalizovaný
- pozice reprezentantů a k nim přiřazených bodů není známá dopředu, k řešení se používá iterativní přístup
- General approach for representative-based algorithms:
  - (většinou náhodná) inicializace k reprezentantů
  - každý bod je přiřazen k nejbližšímu reprezentantovi
  - vytvoření clusterů z bodů přiřazených k reprezentantům
  - nalezení optimálního reprezentanta pro každý cluster $C_j$ za využití minimalizace fce: $\sum_{X_i \in C_j}[Dist(Xi, Yj)]$
- způsob měření vzdálenosti:
  - $L_p$-norm: $Dist(X, Y) = \left \| X-Y \right \|_p^p = (\sum_{i=1}^d|x_i-y_i|^p)^{1/p}$
  - cosinova vzdálenost: $cos(X, Y) = \frac{X \cdot Y }{\left \| X \right \| \cdot \left \| Y \right \|} = \frac{\sum_{i=1}^d(x_i \cdot y_i)}{\sqrt{\sum_{i=1}^d x_i^2} \cdot \sqrt{\sum_{i=1}^d y_i^2})}$

### K-means
- využívá euklidovskou vzdálenost ($L_2$-norm)
- optimální reprezentant clusteru je průměrem – centroidem je prvek, který nemusí být v datasetu
- k-means funguje nejlépe se sférickými clustery

### K-medián
- manhatanovská vzdálenost ($L_1$-norm)
- optimální reprezentant clusteru je medián přes všechny dimenze v clusteru
- je robustnější než k-means, protože medián není tak citlivý na outliery

### K-medoids
- náhodný výběr k reprezentantů – vždy body z datasetu S
- ke každému reprezentantovi nejbližší body – určí se clustery
- v každém clustru se spočítá pro medoid a všechny ostatní body z clusteru cena konfigurace a za medoid se zvolí bod s nejnižší cenou (cena konfigurace = např. Součet délek od bodu ke všem dalším bodům z clusteru)
- většinou zvolené vysoké k a v post procesingu se počet clusterů zmenšuje

## Hierarchické shlukování
vytváří hierarchickou sktrukturu mezi body
### Bottom-up (agglomerative) methods
- iterativní přístup začíná s individuálními objekty, které se shlukují do clusterů vyšší úrovně
- dva clustery jsou spojeny v každé iteraci – každý krok zmenší počet clusterů o jeden
- matice vzdáleností by měla být uložena v paměti, její přepočítávání bude méně náročné
- výsledkem je binární strom clusterů (dendrogram pro znázornění)
- princip citlivý k chybám při spojování – citlivý na noise
- nepraktické pro velké datasety
- Best (single) linkage
  - vzdálenost mezi clustery je rovna minimální vzdálenosti mezi všemi dvojicemi: $D(C_i, C_j) = {min \atop\scriptstyle \forall x \in C_i, \forall y \in C_j} \left \{ d(x,y) \right \}$
  - velmi efektivní přístup ke clusterům různých tvarů
  - velmi citlivý na šum který spojuje dva různé clustery
- Worst (complete) linkage
  - vzdálenost je rovna největší vzdáleností mezi všemi páry: $D(C_i, C_j) = {max \atop\scriptstyle \forall x \in C_i, \forall y \in C_j} \left \{ d(x,y) \right \}$
  - toto kritérium minimalizuje maximální průměr
- Group-average linkage
  - vzdálenost je rovna průměrné vzdálenosti mezi všemi páry, pro výpočet se používá vážená vzdálenost: $D(C_i, C_j) = \frac{1}{|C_i| \cdot |C_j|} \sum_{x \in C_i}\sum_{y \in C_j} d(x,y)$
- Closest centroid
  - centroidy ztrácejí informaci o relativním rozložení bodů
  - metoda nediskriminuje různě velké clustery
- Variance-based criterion
  - minimalizuje odchylku clustrů během spojování
  - spojení vždy způsobí zhoršení kvůli ztráty granularity dat
  - Each cluster maintain 0th, 1st and 2nd order moment statistics.
  - The average squared error of a cluster i is defined as: $SE_i = \sum_{r=1}^d(\frac{S_{ir}}{m_i} - \frac{F_{ir}^2}{m_i^2})$
  - $m_i$ – počet bodů, $S_{ir}$ - squared sum of the data points in a cluster across each direction $r$, $F_{ir}$ suma bodů v daném směru
  - změna rozptylu na clusterech $i$ a $j$ je vždy pozitivní: $\Delta SE_{i \cup j} = SE_{i \cup j} - SE_i - SE_j$
  - pár clusterů s nejmenší změnou rozptylu je vybrán pro spojení
- Ward’s method
  - podobný Variance-based criterion, využívá centroidy
  - velké clustery jsou penalizovány
  - je to neškálovaný součet mocniny error jako slučovací kritérium
  - je to ekvivalent násobení druhé mocniny euklidovské vzdálenost mezi centroidem a harnonickým mean počtu bodů v páru
  - Kritériem pro shlukování je celkový součet druhých mocnin odchylek každého objektu od těžiště (centroidu) shluku, do kterého náleží. Hodí se pro práci s objekty, které mají stejný rozměr proměnných.
  - (z youtube: předstírá, že dva clustery spojil, najde pro tento nový centroid a pak hledá, jak by je nejlépe rozdělil podle toho centroidu)
  - (z netu: vždy využívá Half square Euclidean distance )
### Top-down (divisive) methods
- dělení objektů do stromových struktur
- flat clustering algorithm pro rozdělení
- jeden uzel je rozdělen na několik v jedné iteraci
- strategie výběru uzlu může narušit vyváženost stromu
- pokud je clusterovací podprogram stochastický, testuje se několik pokusů a nejlepší je vybrán
- Bisecting k-Means (půlící k-means)
  - každý uzel je rozdělen na dva potomky 2-means algoritmem
  - několik běhů náhodné inicializace, ten s nejlepším dopadem na clustering je použitý
  - existuje mnoho metod pro výběr uzlu k rozdělení

Kompromis mezi rovnováhou v počtu clusterů a počtu objektů v clusteru
